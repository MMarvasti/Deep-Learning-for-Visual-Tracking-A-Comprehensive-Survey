# Deep Learning for Visual Tracking: A Comprehensive Survey (Extended version of the paper on [arXiv](https://arxiv.org/pdf/1912.00535.pdf))

### The comprehensive comparisons of recent Deep Learning (DL)-based visual tracking methods on the OTB-2013, OTB-2015, VOT-2018, and LaSOT datasets ([Raw Results on OTB Dataset](https://drive.google.com/open?id=1YN8Bhyve0G8wkl4M-rWlnJnkDKKdraO8), [Raw Results on VOT Dataset](https://drive.google.com/open?id=1Nhm5__PoRya8Hdifc6m_v745QbjBNgWN), [Raw Results on LaSOT Dataset](https://drive.google.com/open?id=17kLB5dpy2qg_OtC51ljHLM9ZcaDdhitQ)).

### Comparison will be updated soon with more datasets.!
===============================================================================
### Licence: copyright 2019 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.
===============================================================================
## Performance Comparison of Visual Trackers in terms of Precision and Success Plots on OTB-2013 Dataset (Ranking based on Area Under Curve (AUC)) [[OTB-2013 Dataset]](http://cvlab.hanyang.ac.kr/tracker_benchmark/datasets.html) [[OTB-2013 Paper]](https://ieeexplore.ieee.org/document/6619156): 
**- Average Performance Comparisons of Visual Tracking Methods:**
<img src="OTB2013%20results/1.png"/> 
<img src="OTB2013%20results/13.png"/>
**- Attribute-based Performance Comparisons (Eleven attributes including: Illumination Variation (IV), Scale Variation (SV), Occlusion (OCC), Deformation (DEF), Motion Blur (MB), Fast Motion (FM), In-Plane Rotation (IPR), Out-of-Plane Rotation (OPR), Out-of-View (OV), Background Clutter (BC), Low Resolution (LR)):**
<img src="OTB2013%20results/2.png"/>
<img src="OTB2013%20results/14.png"/>
<img src="OTB2013%20results/3.png"/>
<img src="OTB2013%20results/15.png"/>
<img src="OTB2013%20results/4.png" />
<img src="OTB2013%20results/16.png"/>
<img src="OTB2013%20results/5.png"/>
<img src="OTB2013%20results/17.png"/>
<img src="OTB2013%20results/6.png"/>
<img src="OTB2013%20results/18.png"/>
<img src="OTB2013%20results/7.png"/>
<img src="OTB2013%20results/19.png"/>
<img src="OTB2013%20results/8.png"/>
<img src="OTB2013%20results/20.png"/>
<img src="OTB2013%20results/9.png"/>
<img src="OTB2013%20results/21.png"/>
<img src="OTB2013%20results/10.png"/>
<img src="OTB2013%20results/22.png"/>
<img src="OTB2013%20results/11.png"/>
<img src="OTB2013%20results/23.png"/>
<img src="OTB2013%20results/12.png"/>
<img src="OTB2013%20results/24.png"/>

## Performance Comparison of Visual Trackers in terms of Precision and Success Plots on OTB-2015 Dataset (Ranking based on Area Under Curve (AUC)) [[OTB-2015 Dataset]](http://cvlab.hanyang.ac.kr/tracker_benchmark/datasets.html)[[OTB-2015 Paper]](https://ieeexplore.ieee.org/document/7001050):
**- Average Performance Comparisons of Visual Tracking Methods:**
<img src="OTB2015%20results/1.png"/> 
<img src="OTB2015%20results/13.png"/>
**- Attribute-based Performance Comparisons (Eleven attributes including: Illumination Variation (IV), Scale Variation (SV), Occlusion (OCC), Deformation (DEF), Motion Blur (MB), Fast Motion (FM), In-Plane Rotation (IPR), Out-of-Plane Rotation (OPR), Out-of-View (OV), Background Clutter (BC), Low Resolution (LR)):**
<img src="OTB2015%20results/2.png"/>
<img src="OTB2015%20results/14.png"/>
<img src="OTB2015%20results/3.png"/>
<img src="OTB2015%20results/15.png"/>
<img src="OTB2015%20results/4.png" />
<img src="OTB2015%20results/16.png"/>
<img src="OTB2015%20results/5.png"/>
<img src="OTB2015%20results/17.png"/>
<img src="OTB2015%20results/6.png"/>
<img src="OTB2015%20results/18.png"/>
<img src="OTB2015%20results/7.png"/>
<img src="OTB2015%20results/19.png"/>
<img src="OTB2015%20results/8.png"/>
<img src="OTB2015%20results/20.png"/>
<img src="OTB2015%20results/9.png"/>
<img src="OTB2015%20results/21.png"/>
<img src="OTB2015%20results/10.png"/>
<img src="OTB2015%20results/22.png"/>
<img src="OTB2015%20results/11.png"/>
<img src="OTB2015%20results/23.png"/>
<img src="OTB2015%20results/12.png"/>
<img src="OTB2015%20results/24.png"/>

##  Performance Comparison of Visual Trackers on VOT-2018 Dataset [[VOT-2018 Dataset]](http://www.votchallenge.net/vot2018/dataset.html)[[VOT-2018 Paper]](https://link.springer.com/chapter/10.1007/978-3-030-11009-3_1):
## Experiment Baseline (Expected Overlap Analysis):
**- Expected overlap curves:**
<img src="VOT2018%20results/Expected overlap curves for baseline_Expected overlap analysis_Experiment baseline.png"/>

**- Expected overlap scores:**
<img src="VOT2018%20results/Expected overlap scores for baseline_Expected overlap analysis_Experiment baseline.png"/>

**- Overview: Expected Overlap Analysis:**
<img src="VOT2018%20results/Overview Expected Overlap Analysis Experiment Analysis.png"/>

## Experiment Baseline (Accuracy-Robustness (AR) Ranking):
**- AR plot (mean):**
<img src="VOT2018%20results/AR plot for experiment baseline (mean)_Accuracy-Robustness_Experiment baseline.png"/>

**- AR plot (weighted_mean):**
<img src="VOT2018%20results/AR plot for experiment baseline (weighted_mean)_Accuracy-Robustness_Experiment baseline.png"/>

**- AR plot (pooled):**
<img src="VOT2018%20results/AR plot for experiment baseline (pooled)_Accuracy-Robustness_Experiment baseline.png"/>

**- Table: Accuracy:**
<img src="VOT2018%20results/Accuracy Table Accuracy-Robustness Experiment Baseline.png"/>

**- Table: Robustness:**
<img src="VOT2018%20results/Robustness Table Accuracy-Robustness Experiment Baseline.png"/>

## Experiment Baseline (Attribute-based Ranking: Camera Motion, Illumination Change, Motion Change, Occlusion, Size Change, No Degradation):
**- Orderings for overall overlap:**
<img src="VOT2018%20results/Orderings for overall overlap_Accuracy-Robustness_Experiment baseline.png"/>

**- Orderings for failures:**
<img src="VOT2018%20results/Orderings for failures_Accuracy-Robustness_Experiment baseline.png"/>

**- AR plot for camera motion:**
<img src="VOT2018%20results/AR plot for camera motion in experiment baseline_Accuracy-Robustness_Experiment baseline.png"/> 

**- AR plot for illumination change:**
<img src="VOT2018%20results/AR plot for illumination change in experiment baseline_Accuracy-Robustness_Experiment baseline.png"/>

**- AR plot for motion change:**
<img src="VOT2018%20results/AR plot for motion change in experiment baseline_Accuracy-Robustness_Experiment baseline.png"/>

**- AR plot for occlusion:**
<img src="VOT2018%20results/AR plot for occlusion in experiment baseline_Accuracy-Robustness_Experiment baseline.png"/>

**- AR plot for size change:**
<img src="VOT2018%20results/AR plot for size change in experiment baseline_Accuracy-Robustness_Experiment baseline.png"/>

**- AR plot for no degradation:**
<img src="VOT2018%20results/AR plot for no degradation in experiment baseline_Accuracy-Robustness_Experiment baseline.png" />

## Experiment Baseline (Speed Report) [first to third methods are shown by yellow, blue, and green colors.]:
**- Raw Frame per Second (FPS):**
<img src="VOT2018%20results/BaselineSpeedVOT-page-001.jpg" />

**- Normalized (Equivalent Filter Operations (EFO): "tracker speed in terms of a predefined filtering operation that the VOT tookit automatically carries out prior to running the experiments"):**
<img src="VOT2018%20results/BaselineSpeedVOTEFO-page-001.jpg" />

## Experiment Unsupervised (Overall Comparisons):
**- Average overlap:**
<img src="VOT2018%20results/Experiment unsupervised (average)_Experiment unsupervised.png"/>

**- Table: Overlap Overview:**
<img src="VOT2018%20results/Overlap Overview Experiment Unsupervised.png"/>

**- Orderings for overall overlap:**
<img src="VOT2018%20results/Orderings for overall overlap_Experiment unsupervised.png"/>

## Experiment Unsupervised (Attribute-based Comparisons):
**- Overlap plot for camera motion:**
<img src="VOT2018%20results/Overlap plot for tag tag_camera_motion in experiment unsupervised.png"/>

**- Overlap plot for illum change:**
<img src="VOT2018%20results/Overlap plot for tag tag_illum_change in experiment unsupervised.png"/>

**- Overlap plot for motion change:**
<img src="VOT2018%20results/Overlap plot for tag tag_motion_change in experiment unsupervised.png"/>

**- Overlap plot for occlusion:**
<img src="VOT2018%20results/Overlap plot for tag tag_occlusion in experiment unsupervised.png"/>

**- Overlap plot for size change:**
<img src="VOT2018%20results/Overlap plot for tag tag_size_change in experiment unsupervised.png"/>

**- Overlap plot for no degradation:**
<img src="VOT2018%20results/Overlap plot for tag tag_empty in experiment unsupervised.png"/>

## Experiment Unsupervised (Speed Report): [first to third methods are shown by yellow, blue, and green colors.]:
**- Raw Frame per Second (FPS):**
<img src="VOT2018%20results/UnsupervisedSpeed-page-001.jpg" />

**- Normalized (Equivalent Filter Operations (EFO): "tracker speed in terms of a predefined filtering operation that the VOT tookit automatically carries out prior to running the experiments"):**
<img src="VOT2018%20results/UnsupervisedSpeedVOTEFO-page-001.jpg" />

## Overall Comparison:
**- Report Overview:**
<img src="VOT2018%20results/Report Overview.png"/>

## Qualitative Comparisons of State-of-the-art Visual Tracking Methods on VOT2018 Dataset (Under [TraX Protocol](https://www.sciencedirect.com/science/article/pii/S0925231217303065)):
[BMX_Video](https://www.youtube.com/watch?v=M4GVQZt7MnU), 
[Crabs1_Video](https://www.youtube.com/watch?v=NfpM9BqAaOo), 
[Gymnastics3_Video](https://www.youtube.com/watch?v=fB9S314JZnc),
[Motorcross2_Video](https://www.youtube.com/watch?v=MfveEsYkImw),
[Singer3_Video](https://www.youtube.com/watch?v=iUPZqkkqu7Y),
[Godfather_Video](https://www.youtube.com/watch?v=HKdQpRZGNfw),
[Bag_Video](https://www.youtube.com/watch?v=NiPJs-pKiR0),
[Dinasaur_Video](https://www.youtube.com/watch?v=GEAQluHbf2o),
[Matrix_Video](https://www.youtube.com/watch?v=_l9FxuvHWis),
[Hand_Video](https://www.youtube.com/watch?v=GVbVyvDvRSE),
[Glove_Video](https://www.youtube.com/watch?v=ORnrJzywLCA),
[Ball2_Video](https://www.youtube.com/watch?v=PBUQCcWHN_0),
[Blanket_Video](https://www.youtube.com/watch?v=aznGIj0g88Q),
[Gymnastics1_Video](https://www.youtube.com/watch?v=IQ5LyyScnlM),
[Butterfly_Video](https://www.youtube.com/watch?v=oy2OonlGrIw),
[Motorcross1_Video](https://www.youtube.com/watch?v=U6wSJ6fX8gk),
[Pedestrian_Video](https://www.youtube.com/watch?v=k0NIvzLCdDk),
[Singer2_Video](https://www.youtube.com/watch?v=xuPpytH8Qps),
[Shaking_Video](https://www.youtube.com/watch?v=8oIPuZy4DH0),
[Racing_Video](https://www.youtube.com/watch?v=6Fh-vW4vRHY),
[Handball1_Video](https://www.youtube.com/watch?v=jW7TTJqbjtI),
[Sheep_Video](https://www.youtube.com/watch?v=qDmKJm3usf4),
[Bolt1_Video](https://www.youtube.com/watch?v=EtXuGAsQWJo),
[Fernando_Video](https://www.youtube.com/watch?v=P_nKkfL6DIc),
[Bolt2_Video](https://www.youtube.com/watch?v=CxAYVz9qkUk),
[Book_Video](https://www.youtube.com/watch?v=GJa7q-rmaxA),
[Leaves_Video](https://www.youtube.com/watch?v=oA_Bawv8SaA),
[Fish1_Video](https://www.youtube.com/watch?v=FU1r4JXgfyM),
[Fish2_Video](https://www.youtube.com/watch?v=yQF9VuIh1Yg),
[Tiger_Video](https://www.youtube.com/watch?v=LG3lenIEOC8),
[Wiper_Video](https://www.youtube.com/watch?v=AABuilw6fJk),
[Traffic_Video](https://www.youtube.com/watch?v=_wRB6ZiitcY),
[Crossing_Video](https://www.youtube.com/watch?v=U4_WXqTxTKY),
[Fish3_Video](https://www.youtube.com/watch?v=JReWhzvSkug),
[Ball1_Video](https://www.youtube.com/watch?v=kPrrhZGVl_U),
[Graduate_Video](https://www.youtube.com/watch?v=5Q38SEF3hkA),
[Iceskater_Video](https://www.youtube.com/watch?v=TQrMO25mt50),
[Soldier_Video](https://www.youtube.com/watch?v=Kt-gR67tqm0),
[DroneAcross_Video](https://www.youtube.com/watch?v=vX7WLTvKa_M),
[Soccer2_Video](https://www.youtube.com/watch?v=u9u92BSiSIc),
[DroneFlip_Video](https://www.youtube.com/watch?v=RDRaX0PVc1Y),
[Ants1_Video](https://www.youtube.com/watch?v=ANOM-he1nEs),
[Iceskater_Video](https://www.youtube.com/watch?v=wGYKt8TGkc4),
[Handball2_Video](https://www.youtube.com/watch?v=ZDsIgEdELps),
[Nature_Video](https://www.youtube.com/watch?v=Xmy8diVwD8s),
[Ants3_Video](https://www.youtube.com/watch?v=RXsBGQYeglI),
[Road_Video](https://www.youtube.com/watch?v=inT-VDI9BpY),
[Helicopter_Video](https://www.youtube.com/watch?v=hKW42Wy43CA),
[Girl_Video](https://www.youtube.com/watch?v=FS3ZmQLuUo8),
[Gymnastics2_Video](https://www.youtube.com/watch?v=oRyLZW4GsSo),
[Conduction_Video](https://www.youtube.com/watch?v=80T2zvQhOnU),
[Zebrafish1_Video](https://www.youtube.com/watch?v=VwAr9rnqXQ8),
[Basketball_Video](https://www.youtube.com/watch?v=pDR5n7v82BI),
[Frisbee_Video](https://www.youtube.com/watch?v=lgzWuEBKvSY),
[Car1_Video](https://www.youtube.com/watch?v=sfArINezfzo),
[Birds1_Video](https://www.youtube.com/watch?v=sromxvJbQhs),
[Drone1_Video](https://www.youtube.com/watch?v=RvmgMrItjmI),
[Flamingo_Video](https://www.youtube.com/watch?v=n0HDvwaMJaY).
<img src="VOT2018%20results/Fig8-1.jpg"/>

## Performance Comparison of Visual Trackers in terms of Precision and Success Plots on LaSOT Dataset (Ranking based on Area Under Curve (AUC)) [[LaSOT Dataset]](https://cis.temple.edu/lasot/)[[LaSOT Paper]](https://arxiv.org/abs/1809.07845):
**- Average Performance Comparisons of Visual Tracking Methods:**
<img src="LaSOT%20results/1.png"/> 
<img src="LaSOT%20results/16.png"/> 
**- Attribute-based Performance Comparisons (Fourteen attributes including: Illumination Variation (IV), Scale Variation (SV), Deformation (DEF), Motion Blur (MB), Fast Motion (FM), Out-of-View (OV), Background Clutter (BC), Low Resolution (LR), Aspect Ratio Change (ARC), Camera Motion (CM), Full Occlusion (FOC), Partial Occlusion (POC), Viewpoint Change (VC), Rotation (ROT)):**
<img src="LaSOT%20results/2.png"/> 
<img src="LaSOT%20results/17.png"/> 
<img src="LaSOT%20results/3.png"/> 
<img src="LaSOT%20results/18.png"/> 
<img src="LaSOT%20results/4.png"/> 
<img src="LaSOT%20results/19.png"/> 
<img src="LaSOT%20results/5.png"/> 
<img src="LaSOT%20results/20.png"/> 
<img src="LaSOT%20results/6.png"/> 
<img src="LaSOT%20results/21.png"/> 
<img src="LaSOT%20results/7.png"/> 
<img src="LaSOT%20results/22.png"/> 
<img src="LaSOT%20results/8.png"/> 
<img src="LaSOT%20results/23.png"/> 
<img src="LaSOT%20results/9.png"/> 
<img src="LaSOT%20results/24.png"/> 
<img src="LaSOT%20results/10.png"/> 
<img src="LaSOT%20results/25.png"/> 
<img src="LaSOT%20results/11.png"/> 
<img src="LaSOT%20results/26.png"/> 
<img src="LaSOT%20results/12.png"/> 
<img src="LaSOT%20results/27.png"/> 
<img src="LaSOT%20results/13.png"/> 
<img src="LaSOT%20results/28.png"/> 
<img src="LaSOT%20results/14.png"/> 
<img src="LaSOT%20results/29.png"/> 
<img src="LaSOT%20results/15.png"/> 
<img src="LaSOT%20results/30.png"/> 

## References (Experimentally Evaluated Visual Tracking Methods):
[1] [C. Ma, J. B. Huang, X. Yang, and M. H. Yang, “Hierarchical convolutional features for visual tracking,” in Proc. IEEE ICCV, 2015, pp. 3074–3082. [HCFT]](https://ieeexplore.ieee.org/document/7410709)<br/>

[2] [M. Danelljan, G. Hager, F. S. Khan, and M. Felsberg, “Convolutional features for correlation filter based visual tracking,” in Proc. IEEE ICCVW, 2016, pp. 621–629. [DeepSRDCF]](https://www.cv-foundation.org/openaccess/content_iccv_2015_workshops/w14/papers/Danelljan_Convolutional_Features_for_ICCV_2015_paper.pdf)<br/>

[3] [M. Danelljan, A. Robinson, F. S. Khan, and M. Felsberg, “Beyond correlation filters: Learning continuous convolution operators for visual tracking,” in Proc. ECCV, vol. 9909 LNCS, 2016, pp. 472–488. [CCOT]](https://link.springer.com/chapter/10.1007/978-3-319-46454-1_29)<br/>

[4] [L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and P. H. Torr, “Fully-convolutional Siamese networks for object tracking,” in Proc. ECCV, 2016, pp. 850–865. [SiamFC]](http://www.robots.ox.ac.uk/~luca/siamese-fc.html)<br/>

[5] [R. Tao, E. Gavves, and A.W. Smeulders, “Siamese instance search for tracking,” in Proc. IEEE CVPR, 2016, pp. 1420–1429. [SINT]](https://ieeexplore.ieee.org/document/7780527)<br/>

[6] [H. Nam and B. Han, “Learning multi-domain convolutional neural networks for visual tracking,” in Proc. IEEE CVPR, 2016, pp. 4293–4302. [MDNet]](https://ieeexplore.ieee.org/document/7780834)<br/>

[7] [Y. Qi, S. Zhang, L. Qin, H. Yao, Q. Huang, J. Lim, and M. H. Yang, “Hedged deep tracking,” in Proc. IEEE CVPR, 2016, pp. 4303–4311. [HDT]](https://ieeexplore.ieee.org/document/7780835)<br/>

[8] [H. Fan and H. Ling, “Parallel tracking and verifying: A framework for real-time and high accuracy visual tracking,” in Proc. IEEE ICCV, 2017, pp. 5487–5495. [PTAV]](http://openaccess.thecvf.com/content_ICCV_2017/papers/Fan_Parallel_Tracking_and_ICCV_2017_paper.pdf)<br/>

[9] [H. Fan and H.Ling, “Parallel tracking and verifying,” IEEE Trans. Image Process., vol. 28, no. 8, pp. 4130–4144, 2019. [PTAV]](https://ieeexplore.ieee.org/document/8667882)<br/>

[10] [Z. Zhu, G. Huang,W. Zou, D. Du, and C. Huang, “UCT: Learning unified convolutional networks for real-time visual tracking,” in Proc. ICCVW, 2018, pp. 1973–1982. [UCT]](http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w28/Zhu_UCT_Learning_Unified_ICCV_2017_paper.pdf)<br/>

[11] [Q. Guo, W. Feng, C. Zhou, R. Huang, L. Wan, and S. Wang, “Learning dynamic Siamese network for visual object tracking,” in Proc. IEEE ICCV, 2017, pp. 1781–1789. [DSiam]](http://openaccess.thecvf.com/content_ICCV_2017/papers/Guo_Learning_Dynamic_Siamese_ICCV_2017_paper.pdf)<br/>

[12] [J. Valmadre, L. Bertinetto, J. Henriques, A. Vedaldi, and P. H. Torr, “End-to-end representation learning for correlation filter based tracking,” in Proc. IEEE CVPR, 2017, pp. 5000–5008. [CFNet]](https://ieeexplore.ieee.org/document/8100014)<br/>

[13] [M. Danelljan, G. Bhat, F. Shahbaz Khan, and M. Felsberg, “ECO: Efficient convolution operators for tracking,” in Proc. IEEE CVPR, 2017, pp. 6931–6939. [ECO]](https://ieeexplore.ieee.org/document/8100216)<br/>

[14] [A. Lukezˇicˇ, T. Voj´ırˇ, L. Cˇ ehovin Zajc, J. Matas, and M. Kristan, “Discriminative correlation filter tracker with channel and spatial reliability,” IJCV, vol. 126, no. 7, pp. 671–688, 2018. [DeepCSRDCF]](https://link.springer.com/article/10.1007/s11263-017-1061-3)<br/>

[15] [T. Zhang, C. Xu, and M. H. Yang, “Multi-task correlation particle filter for robust object tracking,” in Proc. IEEE CVPR, 2017, pp. 4819–4827. [MCPF]](https://ieeexplore.ieee.org/document/8099995)<br/>

[16] [J. Choi, H. J. Chang, S. Yun, T. Fischer, Y. Demiris, and J. Y. Choi, “Attentional correlation filter network for adaptive visual tracking,” in Proc. IEEE CVPR, 2017, pp. 4828–4837. [ACFN]](https://ieeexplore.ieee.org/document/8099996)<br/>

[17] [Q. Wang, J. Gao, J. Xing, M. Zhang, and W. Hu, “DCFNet: Discriminant correlation filters network for visual tracking,” 2017. [Online]. [DCFNet][DCFNet2]](http://arxiv.org/abs/1704.04057)<br/>

[18] [X. Dong and J. Shen, “Triplet loss in Siamese network for object tracking,” in Proc. ECCV, vol. 11217 LNCS, 2018, pp. 472–488. [TripletLoss-CFNet][TripletLoss-SiamFC][TripletLoss-CFNet2]](https://link.springer.com/chapter/10.1007/978-3-030-01261-8_28)<br/>

[19] [G. Bhat, J. Johnander, M. Danelljan, F. S. Khan, and M. Felsberg, “Unveiling the power of deep tracking,” in Proc. ECCV, 2018, pp. 493–509. [UPDT]](https://link.springer.com/chapter/10.1007/978-3-030-01216-8_30)<br/>

[20] [Z. Zhu, Q. Wang, B. Li, W. Wu, J. Yan, and W. Hu, “Distractor-aware Siamese networks for visual object tracking,” in Proc. ECCV, vol. 11213 LNCS, 2018, pp. 103–119. [DaSiamRPN]](https://link.springer.com/chapter/10.1007/978-3-030-01240-3_7)<br/>

[21] [Y. Zhang, L. Wang, J. Qi, D. Wang, M. Feng, and H. Lu, “Structured Siamese network for real-time visual tracking,” in Proc. ECCV, 2018, pp. 355–370. [StructSiam]](https://link.springer.com/chapter/10.1007/978-3-030-01240-3_22)<br/>

[22] [H. Morimitsu, “Multiple context features in Siamese networks for visual object tracking,” in Proc. ECCVW, 2019, pp. 116–131. [Siam-MCF]](https://link.springer.com/chapter/10.1007/978-3-030-11009-3_6)<br/>

[23] [J. Choi, H. J. Chang, T. Fischer, S. Yun, K. Lee, J. Jeong, Y. Demiris, and J. Y. Choi, “Context-aware deep feature compression for high-speed visual tracking,” in Proc. IEEE CVPR, 2018, pp. 479–488. [TRACA]](https://ieeexplore.ieee.org/document/8578155)<br/>

[24] [Y. Song, C. Ma, X. Wu, L. Gong, L. Bao, W. Zuo, C. Shen, R. W. Lau, and M. H. Yang, “VITAL: Visual tracking via adversarial learning,” in Proc. IEEE CVPR, 2018, pp. 8990–8999. [VITAL]](https://ieeexplore.ieee.org/document/8579035)<br/>

[25] [F. Li, C. Tian, W. Zuo, L. Zhang, and M. H. Yang, “Learning spatial-temporal regularized correlation filters for visual tracking,” in Proc. IEEE CVPR, 2018, pp. 4904–4913. [DeepSTRCF]](https://ieeexplore.ieee.org/document/8578613)<br/>

[26] [B. Li, J. Yan, W. Wu, Z. Zhu, and X. Hu, “High performance visual tracking with Siamese region proposal network,” in Proc. IEEE CVPR, 2018, pp. 8971–8980. [SiamRPN]](https://ieeexplore.ieee.org/document/8579033)<br/>

[27] [A. He, C. Luo, X. Tian, andW. Zeng, “A twofold Siamese network for real-time object tracking,” in Proc. IEEE CVPR, 2018, pp. 4834–4843. [SA-Siam]](https://ieeexplore.ieee.org/document/8578606)<br/>

[28] [C. Sun, D. Wang, H. Lu, and M. H. Yang, “Learning spatial-aware regressions for visual tracking,” in Proc. IEEE CVPR, 2018, pp. 8962–8970. [LSART]](https://ieeexplore.ieee.org/document/8579032)<br/>

[29] [C. Sun, D. Wang, H. Lu, and M. H. Yang, “Correlation tracking via joint discrimination and reliability learning,” in Proc. IEEE CVPR, 2018, pp. 489–497. [DRT]](https://ieeexplore.ieee.org/document/8578156)<br/>

[30] [S. Pu, Y. Song, C. Ma, H. Zhang, and M. H. Yang, “Deep attentive tracking via reciprocative learning,” in Proc. NIPS, 2018, pp. 1931–1941. [DAT]](http://papers.nips.cc/paper/7463-deep-attentive-tracking-via-reciprocative-learning)<br/>

[31] [C. Ma, J. B. Huang, X. Yang, and M. H. Yang, “Robust visual tracking via hierarchical convolutional features,” IEEE Trans. Pattern Anal. Mach. Intell., 2018. [HCFTs]](https://ieeexplore.ieee.org/document/8434334)<br/>

[32] [C. Ma, J. B. Huang, X. Yang, and M. H. Yang, “Adaptive correlation filters with long-term and short-term memory for object tracking,” IJCV, vol. 126, no. 8, pp. 771–796, 2018. [LCTdeep]](https://link.springer.com/article/10.1007/s11263-018-1076-4)<br/>

[33] [E. Gundogdu and A. A. Alatan, “Good features to correlate for visual tracking,” IEEE Trans. Image Process., vol. 27, no. 5, pp. 2526–2540, 2018. [CFCF]](https://ieeexplore.ieee.org/document/8291524)<br/>

[34] [H. Fan and H. Ling, “Siamese cascaded region proposal networks for real-time visual tracking,” in Proc. IEEE CVPR, 2019. [C-RPN]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Fan_Siamese_Cascaded_Region_Proposal_Networks_for_Real-Time_Visual_Tracking_CVPR_2019_paper.pdf)<br/>

[35] [J. Gao, T. Zhang, and C. Xu, “Graph convolutional tracking,” in Proc. CVPR, 2019, pp. 4649–4659. [GCT]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Gao_Graph_Convolutional_Tracking_CVPR_2019_paper.pdf)<br/>

[36] [Q. Wang, L. Zhang, L. Bertinetto, W. Hu, and P. H. S. Torr, “Fast online object tracking and segmentation: A unifying approach,” in Proc. IEEE CVPR, 2019. [SiamMask]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Fast_Online_Object_Tracking_and_Segmentation_A_Unifying_Approach_CVPR_2019_paper.pdf)<br/>

[37] [B. Li, W. Wu, Q. Wang, F. Zhang, J. Xing, and J. Yan, “SiamRPN++: Evolution of Siamese visual tracking with very deep networks,” in Proc. IEEE CVPR, 2019. [SiamRPN++]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_SiamRPN_Evolution_of_Siamese_Visual_Tracking_With_Very_Deep_Networks_CVPR_2019_paper.pdf)<br/>

[38] [X. Li, C. Ma, B. Wu, Z. He, and M.-H. Yang, “Target-aware deep tracking,” in Proc. IEEE CVPR, 2019. [TADT]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Target-Aware_Deep_Tracking_CVPR_2019_paper.pdf)<br/>

[39] [K. Dai, D. Wang, H. Lu, C. Sun, and J. Li, “Visual tracking via adaptive spatially-regularized correlation filters,” in Proc. CVPR, 2019, pp. 4670–4679. [ASRCF]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Dai_Visual_Tracking_via_Adaptive_Spatially-Regularized_Correlation_Filters_CVPR_2019_paper.pdf)<br/>

[40] [Z. Zhang and H. Peng, “Deeper and wider Siamese networks for real-time visual tracking,” in Proc. IEEE CVPR, 2019. [SiamDW-SiamRPN][SiamDW-SiamFC]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Deeper_and_Wider_Siamese_Networks_for_Real-Time_Visual_Tracking_CVPR_2019_paper.pdf)<br/>

[41] [Y. Song, C. Ma, L. Gong, J. Zhang, R. W. Lau, and M. H. Yang, “CREST: Convolutional residual learning for visual tracking,” in Proc. ICCV, 2017, pp. 2574–2583. [CREST][Meta-CREST]](https://ieeexplore.ieee.org/document/8237541)<br/>
